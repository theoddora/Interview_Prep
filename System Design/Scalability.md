# Introduction
The simplest and quickest way to increase the capacity is to *scale
up* the machine hosting the application.

The alternative to scaling up is to *scale out* by distributing the application across multiple nodes. For example, we
can move the database to a dedicated machine as a first step. By
doing that, we have increased the capacity of both the server and
the database since they no longer have to compete for resources.
This is an example of a more general pattern called *functional decomposition*: breaking down an application into separate components,
each with its own well-defined responsibility.

There are other
two general patterns that we can exploit (and combine) to build
scalable applications: splitting data into partitions and distributing them among nodes (*partitioning*) and replicating functionality
or data across nodes, also known as horizontal scaling (*replication*).

# HTTP caching
A static resource contains data that usually
doesn’t change from one request to another, like a JavaScript or
CSS file. Instead, a dynamic resource is generated by the server
on the fly, like a JSON document containing the user’s profile.

Since a static resource doesn’t change often, it can be cached. The
idea is to have a client (i.e., a browser) cache the resource for some
time so that the next access doesn’t require a network call, reducing
the load on the server and the response time.

HTTP caching is limited to safe request meth-
ods that don’t alter the server’s state, like GET or HEAD. Suppose
a client issues a GET request for a resource it hasn’t accessed before. The local cache intercepts the request, and if it can’t find the
resource locally, it will go and fetch it from the origin server on
behalf of the client.

The server uses specific HTTP response headers to let clients
know that a resource is cachable. So when the server responds with the resource, it adds a *Cache-Control* header that defines for
how long to cache the resource (TTL) and an *ETag* header with a
version identifier. Finally, when the cache receives the response,
it stores the resource locally and returns it to the client.

Now, suppose some time passes and the client tries to access the
resource again. The cache first checks whether the resource hasn’t
expired yet, i.e., whether it’s *fresh*. If so, the cache immediately
returns it. However, even if the resource hasn’t expired from the
client’s point of view, the server may have updated it in the meantime. That means reads are not strongly consistent, but we will
safely assume that this is an acceptable tradeoff for our application.

If the resource has expired, it’s considered stale. In this case, the cache sends a GET request to the server with a conditional header
(like *If-None-Match*) containing the version identifier of the stale
resource to check whether a newer version is available. If there is,
the server returns the updated resource; if not, the server replies
with a 304 Not Modified status code.

Ideally, a static resource should be immutable so that clients can
cache it “forever,” which corresponds to a maximum length of a
year according to the HTTP specification. We can still modify a
static resource if needed by creating a new one with a different
URL so that clients will be forced to fetch it from the server.

Another advantage of treating static resources as immutable is that
we can update multiple related resources atomically. For example,
if we publish a new version of the application’s website, the up-
dated HTML index file is going to reference the new URLs for the
JavaScript and CSS bundles. Thus, a client will see either the old version of the website or the new one depending on which index
file it has read, but never a mix of the two that uses, e.g., the old
JavaScript bundle with the new CSS one.

Another way of thinking about HTTP caching is that we treat
the read path (GET) differently from the write path (POST, PUT,
DELETE) because we expect the number of reads to be several
orders of magnitude higher than the number of writes. This is a
common pattern referred to as the *Command Query Responsibility
Segregation* (CQRS) pattern.

To summarize, allowing clients to cache static resources has reduced the load on our server, and all we had to do was to play
with some HTTP headers! We can take caching one step further
by introducing a server-side HTTP cache with a reverse proxy.

## Reverse proxies
A reverse proxy is a server-side proxy that intercepts all communications with clients. Since the proxy is indistinguishable from
the actual server, clients are unaware that they are communicating
through an intermediary.

A common use case for a reverse proxy is to cache static resources
returned by the server. Since the cache is shared among the clients,
it will decrease the load of the server a lot more than any client-side
cache ever could.

Because a reverse proxy is a middleman, it can be used for much
more than just caching. For example, it can:
* authenticate requests on behalf of the server;
* compress a response before returning it to the client to speed
up the transmission;
* rate-limit requests coming from specific IPs or users to protect the server from being overwhelmed;
* load-balance requests across multiple servers to handle more
load.

NGINX and HAProxy are widely-used reverse proxies that we could
leverage to build a server-side cache. However, many use cases for
reverse proxies have been commoditized by managed services. So,
rather than building out a server-side cache, we could just leverage
a Content Delivery Network (CDN).

| Aspect               | Proxy                                    | Reverse Proxy                                     |
|----------------------|------------------------------------------|---------------------------------------------------|
| **Definition**       | Forwards client requests to the internet, masking client identity. | Sits between clients and servers, forwarding requests from clients to backend servers. |
| **Primary Use**      | Used by clients to hide their IP addresses, access restricted content, or add anonymity. | Used by servers to load balance, improve security, and handle client requests efficiently. |
| **Location**         | Positioned between the client and external network (e.g., the internet). | Positioned between the internet (or clients) and internal servers. |
| **Example Use Cases**| Web browsing with anonymity, accessing geo-restricted content. | Load balancing, caching, SSL termination, hiding internal server structure. |
| **Request Flow**     | Client -> Proxy -> Server (Internet) -> Proxy -> Client | Client -> Reverse Proxy -> Server -> Reverse Proxy -> Client |
| **IP Address Seen by Server** | Proxy’s IP address (masks client's IP) | Reverse proxy’s IP address (client’s IP is hidden) |
| **Common Technologies** | Squid, Privoxy | Nginx, HAProxy, Apache HTTP Server |
| **Security Role**    | Primarily focused on client privacy | Adds a security layer by controlling and filtering incoming requests to servers |

The difference between a forward and reverse proxy is subtle but important. A simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.

# Content delivery networks
