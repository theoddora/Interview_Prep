# Introduction
The simplest and quickest way to increase the capacity is to *scale
up* the machine hosting the application.

The alternative to scaling up is to *scale out* by distributing the application across multiple nodes. For example, we
can move the database to a dedicated machine as a first step. By
doing that, we have increased the capacity of both the server and
the database since they no longer have to compete for resources.
This is an example of a more general pattern called *functional decomposition*: breaking down an application into separate components,
each with its own well-defined responsibility.

There are other
two general patterns that we can exploit (and combine) to build
scalable applications: splitting data into partitions and distributing them among nodes (*partitioning*) and replicating functionality
or data across nodes, also known as horizontal scaling (*replication*).

# HTTP caching
A static resource contains data that usually
doesn’t change from one request to another, like a JavaScript or
CSS file. Instead, a dynamic resource is generated by the server
on the fly, like a JSON document containing the user’s profile.

Since a static resource doesn’t change often, it can be cached. The
idea is to have a client (i.e., a browser) cache the resource for some
time so that the next access doesn’t require a network call, reducing
the load on the server and the response time.

HTTP caching is limited to safe request meth-
ods that don’t alter the server’s state, like GET or HEAD. Suppose
a client issues a GET request for a resource it hasn’t accessed before. The local cache intercepts the request, and if it can’t find the
resource locally, it will go and fetch it from the origin server on
behalf of the client.

The server uses specific HTTP response headers to let clients
know that a resource is cachable. So when the server responds with the resource, it adds a *Cache-Control* header that defines for
how long to cache the resource (TTL) and an *ETag* header with a
version identifier. Finally, when the cache receives the response,
it stores the resource locally and returns it to the client.

Now, suppose some time passes and the client tries to access the
resource again. The cache first checks whether the resource hasn’t
expired yet, i.e., whether it’s *fresh*. If so, the cache immediately
returns it. However, even if the resource hasn’t expired from the
client’s point of view, the server may have updated it in the meantime. That means reads are not strongly consistent, but we will
safely assume that this is an acceptable tradeoff for our application.

If the resource has expired, it’s considered stale. In this case, the cache sends a GET request to the server with a conditional header
(like *If-None-Match*) containing the version identifier of the stale
resource to check whether a newer version is available. If there is,
the server returns the updated resource; if not, the server replies
with a 304 Not Modified status code.

Ideally, a static resource should be immutable so that clients can
cache it “forever,” which corresponds to a maximum length of a
year according to the HTTP specification. We can still modify a
static resource if needed by creating a new one with a different
URL so that clients will be forced to fetch it from the server.

Another advantage of treating static resources as immutable is that
we can update multiple related resources atomically. For example,
if we publish a new version of the application’s website, the up-
dated HTML index file is going to reference the new URLs for the
JavaScript and CSS bundles. Thus, a client will see either the old version of the website or the new one depending on which index
file it has read, but never a mix of the two that uses, e.g., the old
JavaScript bundle with the new CSS one.

Another way of thinking about HTTP caching is that we treat
the read path (GET) differently from the write path (POST, PUT,
DELETE) because we expect the number of reads to be several
orders of magnitude higher than the number of writes. This is a
common pattern referred to as the *Command Query Responsibility
Segregation* (CQRS) pattern.

To summarize, allowing clients to cache static resources has reduced the load on our server, and all we had to do was to play
with some HTTP headers! We can take caching one step further
by introducing a server-side HTTP cache with a reverse proxy.

## Reverse proxies
A reverse proxy is a server-side proxy that intercepts all communications with clients. Since the proxy is indistinguishable from
the actual server, clients are unaware that they are communicating
through an intermediary.

A common use case for a reverse proxy is to cache static resources
returned by the server. Since the cache is shared among the clients,
it will decrease the load of the server a lot more than any client-side
cache ever could.

Because a reverse proxy is a middleman, it can be used for much
more than just caching. For example, it can:
* authenticate requests on behalf of the server;
* compress a response before returning it to the client to speed
up the transmission;
* rate-limit requests coming from specific IPs or users to protect the server from being overwhelmed;
* load-balance requests across multiple servers to handle more
load.

NGINX and HAProxy are widely-used reverse proxies that we could
leverage to build a server-side cache. However, many use cases for
reverse proxies have been commoditized by managed services. So,
rather than building out a server-side cache, we could just leverage
a Content Delivery Network (CDN).

| Aspect               | Proxy                                    | Reverse Proxy                                     |
|----------------------|------------------------------------------|---------------------------------------------------|
| **Definition**       | Forwards client requests to the internet, masking client identity. | Sits between clients and servers, forwarding requests from clients to backend servers. |
| **Primary Use**      | Used by clients to hide their IP addresses, access restricted content, or add anonymity. | Used by servers to load balance, improve security, and handle client requests efficiently. |
| **Location**         | Positioned between the client and external network (e.g., the internet). | Positioned between the internet (or clients) and internal servers. |
| **Example Use Cases**| Web browsing with anonymity, accessing geo-restricted content. | Load balancing, caching, SSL termination, hiding internal server structure. |
| **Request Flow**     | Client -> Proxy -> Server (Internet) -> Proxy -> Client | Client -> Reverse Proxy -> Server -> Reverse Proxy -> Client |
| **IP Address Seen by Server** | Proxy’s IP address (masks client's IP) | Reverse proxy’s IP address (client’s IP is hidden) |
| **Common Technologies** | Squid, Privoxy | Nginx, HAProxy, Apache HTTP Server |
| **Security Role**    | Primarily focused on client privacy | Adds a security layer by controlling and filtering incoming requests to servers |

The difference between a forward and reverse proxy is subtle but important. A simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.

# Content delivery networks
A CDN is an overlay network of geographically distributed
caching servers (reverse proxies) architected around the design
limitations of the network protocols that run the internet.

When using a CDN, clients hit URLs that resolve to caching servers
that belong to the CDN. When a CDN server receives a request,
it checks whether the requested resource is cached locally. If not,
the CDN server transparently fetches it from the origin server (i.e.,
our application server) using the original URL, caches the response
locally, and returns it to the client. AWS CloudFront and Akamai
are examples of well-known CDN services.

## Overlay network
You would think that the main benefit of a CDN is caching, but it’s
actually the underlying network substrate. The public internet is
composed of thousands of networks, and its core routing protocol,
BGP, was not designed with performance in mind. It primarily uses the number of hops to cost how expensive a path is with respect to another, without considering their latencies or congestion.

As the name implies, a CDN is a network. More specifically, an
overlay network built on top of the internet that exploits a variety
of techniques to reduce the response time of network requests and
increase the bandwidth of data transfers.

No matter how fast the server is, if the client is located on the other
side of the world from it, the response time is going to be over 100
ms just because of the network latency, which is physically limited
by the speed of light. Not to mention the increased error rate when
sending data across the public internet over long distances.

This is why CDN clusters are placed in multiple geographical locations to be closer to clients. But how do clients know which cluster
is closest to them? One way is via *global DNS load balancing*: an
extension to DNS that considers the location of the client inferred
from its IP, and returns a list of the geographically closest clusters
taking into account also the network congestion and the clusters’
health.

CDN servers are also placed at *internet exchange points*, where ISPs
connect to each other. That way, virtually the entire communication from the origin server to the clients flows over network links
that are part of the CDN, and the brief hops on either end have low
latencies due to their short distance.

The routing algorithms of the overlay network are optimized
to select paths with reduced latencies and congestion, based
on continuously updated data about the health of the network.
Additionally, TCP optimizations are exploited where possible,
such as using pools of persistent connections between servers to avoid the overhead of setting up new connections and using
optimal TCP window sizes to maximize the effective bandwidth.

The overlay network can also be used to speed up the delivery
of dynamic resources that cannot be cached. In this capacity, the
CDN becomes the frontend for the application, shielding it against
distributed denial-of-service (DDoS) attacks.

### Global load balancing
Global server load balancing (GSLB) is the practice of distributing web and application traffic across multiple servers in different locations to improve network performance, increase reliability, and achieve high availability. By directing traffic to the least congested servers or to the servers closest to users, GSLB enables faster and more reliable response times for better user experiences.

## Caching
A CDN can have multiple content caching layers. The top layer is
made of edge clusters deployed at different geographical locations,
as mentioned earlier. But infrequently accessed content might not
be available at the edge, in which case the edge servers must fetch it
from the origin server. Thanks to the overlay network, the content
can be fetched more efficiently and reliably than what the public
internet would allow.

There is a tradeoff between the number of edge clusters and the
cache hit ratio, i.e., the likelihood of finding an object in the cache. The higher the number of edge clusters, the more geographically
dispersed clients they can serve, but the lower the cache hit ratio
will be, and consequently, the higher the load on the origin server.
To alleviate this issue, the CDN can have one or more intermedi-
ary caching clusters deployed in a smaller number of geographical
locations, which cache a larger fraction of the original content.

Within a CDN cluster, the content is partitioned among multiple
servers so that each one serves only a specific subset of it; this is
necessary as no single server would be able to hold all the data.

# Partitioning
When an application’s data keeps growing in size, its volume will
eventually become large enough that it can’t fit on a single machine. To work around that, it needs to be split into partitions, or
*shards*, small enough to fit into individual nodes. As an additional
benefit, the system’s capacity for handling requests increases as
well, since the load of accessing the data is spread over more nodes.

When a client sends a request to a partitioned system, the request
needs to be routed to the node(s) responsible for it. A gateway service (i.e., a reverse proxy) is usually in charge of this, knowing how
the data is mapped to partitions and nodes. This
mapping is generally maintained by a fault-tolerant coordination
service, like etcd or Zookeeper.

Partitioning is not a free lunch since it introduces a fair amount of
complexity:
* A gateway service is required to route requests to the right
nodes.
* To roll up data across partitions, it needs to be pulled from
multiple partitions and aggregated (e.g., think of the complexity of implementing a “group by” operation across partitions).
* Transactions are required to atomically update data that
spans multiple partitions, limiting scalability.
* If a partition is accessed much more frequently than others,
the system’s ability to scale is limited.
* Adding or removing partitions at run time becomes challenging, since it requires moving data across nodes.

It’s no coincidence we are talking about partitioning right after discussing CDNs. A cache lends itself well to partitioning as it avoids
most of this complexity. For example, it generally doesn’t need to
atomically update data across partitions or perform aggregations
spanning multiple partitions.

Now that we have an idea of what partitioning is and why it’s
useful, let’s discuss how data, in the form of key-value pairs, can be
mapped to partitions. At a high level, there are two main ways of
doing that, referred to as *range partitioning* and *hash partitioning*. An
important prerequisite for both is that the number of possible keys
is very large; for example, a boolean key with only two possible
values is not suited for partitioning since it allows for at most two partitions.

## Range partitioning
Range partitioning splits the data by key range into lexicographically sorted partitions. To make range
scans fast, each partition is generally stored in sorted order on disk.

The first challenge with range partitioning is picking the partition
boundaries. For example, splitting the key range evenly makes
sense if the distribution of keys is more or less uniform. If not, like
in the English dictionary, the partitions will be unbalanced and
some will have a lot more entries than others.

Another issue is that some access patterns can lead to *hotspots*,
which affect performance. For example, if the data is range
partitioned by date, all requests for the current day will be served
by a single node. There are ways around that, like adding a
random prefix to the partition keys, but there is a price to pay in
terms of increased complexity.

When the size of the data or the number of requests becomes too
large, the number of nodes needs to be increased to sustain the increase in load. Similarly, if the data shrinks and/or the number
of requests drops, the number of nodes should be reduced to decrease costs. The process of adding and removing nodes to balance the system’s load is called *rebalancing*. Rebalancing has to be implemented in a way that minimizes disruption to the system, which
needs to continue to serve requests. Hence, the amount of data
transferred when rebalancing partitions should be minimized.

One solution is to create a lot more partitions than necessary when
the system is first initialized and assign multiple partitions to each
node. This approach is also called *static partitioning* since the number of partitions doesn’t change over time. When a new node joins,
some partitions move from the existing nodes to the new one so
that the store is always in a balanced state. The drawback of this
approach is that the number of partitions is fixed and can’t be easily changed. Getting the number of partitions right is hard — too
many partitions add overhead and decrease performance, while
too few partitions limit scalability. Also, some partitions might
end up being accessed much more than others, creating hotspots.

The alternative is to create partitions on demand, also referred to
as *dynamic partitioning*. The system starts with a single partition,
and when it grows above a certain size or becomes too hot, the
partition is split into two sub-partitions, each containing approximately half of the data, and one of the sub-partitions is transferred
to a new node. Similarly, when two adjacent partitions become
small or “cold” enough, they can be merged into a single one.

## Hash partitioning
Let’s take a look at an alternative way of mapping data to partitions. The idea is to use a hash function that deterministically
maps a key (string) to a seemingly random number (a hash) within
a certain range (e.g., 0 and 2^64 − 1). This guarantees that the keys’
hashes are distributed uniformly across the range.

Next, we assign a subset of the hashes to each partition. For example, one way of doing that is by taking the
modulo of a hash, i.e., hash(key) mod N, where N is the number of
partitions.

Although this approach ensures that the partitions contain more or less the same number of entries, it doesn’t eliminate hotspots
if the *access pattern* is not uniform. For example, if a single key is
accessed significantly more often than others, the node hosting the
partition it belongs to could become overloaded. In this case, the
partition needs to be split further by increasing the total number
of partitions. Alternatively, the key needs to be split into sub-keys
by, e.g., prepending a random prefix.

Assigning hashes to partitions via the modulo operator can become problematic when a new partition is added, as most keys
have to be moved (or shuffled) to a different partition because their
assignment changed. Shuffling data is extremely expensive as it
consumes network bandwidth and other resources. Ideally, if a
partition is added, only 𝐾/𝑁 keys should be shuffled around, where
𝐾 is the number of keys and 𝑁 is the number of partitions. One
widely used hashing strategy with that property is consistent hashing.

With *consistent hashing*, a hash function randomly maps both the
partition identifiers and keys onto a circle, and each key is assigned
to the closest partition that appears on the circle in clockwise order.

Now, when a new partition is added, only the keys that now map to it on the circle need to be reassigned.

The goal of consistent hashing - we want almost all objects to stay assigned to the same server even as the number of servers changes. 

In addition to hashing the object keys we also add the hash of the server names. The objects and servers are hashed with the same hashing function to the same range of values. 
A hash space: x0 to xn. We connect both ends of the hash space to form a ring. This is a hash ring. Using a hashing function, we hash each server by its name or IP address and place the server on the ring. Next, we hash each object by its keys with the same hashing function. Unlike simple hashing where we perform a modulo operator on the hash, here we use the hash direclty, to map the object key onto the ring.

To locate the server of a particular object, we go clockwise from the location of the object key onto the ring until a server is found. 

What happens when we add a server (example: s4 before s0)? Only the keys which previously poited to s4 need to be moved to point to s0. Keys up until the new position of s4. 

With simple hashing, when a new server is added, almost all the keys need to be remapped. With consistent hashing, adding a new server only requres redistribution of a fraction of the keys. 

What happens when we remove a server? We remap the keys pointing to that server, to the next one in the ring. 

What happens when the server nodes are distributed unevenly on the ring?  It is unlikely to get a perfect partition of the ring into equal sizes. This problem gets worse if servers come and go frequently. 

Virtual nodes are used to fix this problem. The idea is to have each server appear at multiple locations on the ring. Each location is a virtual node representing a server. With virtual nodes, each server handles multiple segments onto the ring. As the number of virtual nodes increases, the distribution of objects becomes more balanced. Having more virtual nodes means taking up more space to store the metadata about the virtual nodes. This is a trade-off, and we can tune the number of virtual nodes to fit our system requirements. 

| Use Case                      | Description                                                                                                  | Benefits of Consistent Hashing                                                   |
|-------------------------------|--------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **Distributed Caching**       | Used in distributed cache systems (e.g., Memcached) to map cache keys to cache servers.                     | Ensures minimal cache reassignments when servers are added/removed, improving cache hit rate. |
| **Load Balancing**            | In systems with dynamic server pools, such as in microservices or cloud environments.                        | Dynamically distributes load, preventing "hot spots" and minimizing reassignments of tasks.    |
| **Distributed Databases**     | Databases like Cassandra and DynamoDB use consistent hashing for data partitioning across nodes.             | Facilitates data partitioning, ensuring fault tolerance and even data distribution.            |
| **Content Delivery Networks** | Maps web content to geographically distributed servers for faster delivery.                                  | Optimizes resource location with minimal redistributions, reducing latency for end-users.      |
| **Peer-to-Peer Networks**     | Used in P2P networks (e.g., BitTorrent, Chord) to distribute file pieces across peers.                      | Balances file sharing load across peers and simplifies node addition/removal.                 |
| **Distributed File Systems**  | Systems like Amazon S3 or HDFS use consistent hashing to distribute files across storage nodes.             | Maintains balanced file distribution, reducing data shuffling when scaling storage.           |
| **Sharded Queues**            | Message queues (e.g., Kafka, RabbitMQ) use it to distribute messages across queue partitions.               | Balances message flow, ensuring minimal re-routing when queue nodes change.                    |
| **Social Media Platforms**    | Used for user content distribution and follower management (e.g., mapping followers to backend servers).    | Provides efficient scaling and avoids disruption when user volume fluctuates.                  |


The main drawback of hash partitioning compared to range partitioning is that the sort order over the partitions is lost, which is
required to efficiently scan all the data in order. However, the data
within an individual partition can still be sorted based on a secondary key.

# File storage
But there are only so many images, videos, etc., the server can store on its local disk(s) before
running out of space. To work around this limit, we can use a managed file store, like AWS S3 or Azure Blob Storage, to store large
static files. Managed file stores are scalable, highly available, and
offer strong durability guarantees. A file uploaded to a managed
store can be configured to allow access to anyone who knows its
URL, which means we can point the CDN straight at it. This allows
us to completely offload the storage and serving of static resources
to managed services.

## Blob storage architecture
Because distributed file stores are such a crucial component of
modern applications, it’s useful to have an idea of how they work
underneath. We will dive into the architecture  of Azure Storage (AS), a scalable cloud storage system that provides
strong consistency. AS supports file, queue, and table abstractions,
but for simplicity, our discussion will focus exclusively on the file
abstraction, also referred to as the blob store.

AS is composed of storage clusters distributed across multiple regions worldwide. A storage cluster is composed of multiple racks
of nodes, where each rack is built out as a separate unit with redundant networking and power.

At a high level, AS exposes a global namespace based on domain
names that are composed of two parts: an account name and a file
name. The two names together form a unique URL that points to
a specific file, e.g., https://ACCOUNT_NAME.blob.core.windows.
net/FILE_NAME. The customer configures the account name, and
the AS DNS server uses it to identify the storage cluster where the
data is stored. The cluster uses the file name to locate the node
responsible for the data.

A central *location service* acts as the global *control plane* in charge
of creating new accounts and allocating them to clusters, and also
moving them from one cluster to another for better load distribution. For example, when a customer wants to create a new account
in a specific region, the location service:
* chooses a suitable cluster to which to allocate the account
based on load information;
updates the configuration of the cluster to start accepting requests for the new account;
* and creates a new DNS record that maps the account name
to the cluster’s public IP address.

From an architectural point of view, a storage cluster is composed
of three layers: a stream layer, a partition layer, and a front-end
layer.

The *stream layer* implements a distributed append-only file system
in which the data is stored in so-called streams. Internally, a *stream* is represented as a sequence of extents, where the extent is the unit
of replication. Writes to extents are replicated synchronously using
chain replication.

The *stream manager* is the control plane responsible for assigning
an extent to a chain of storage servers in the cluster. When the
manager is asked to allocate a new extent, it replies with the list of
storage servers that hold a copy of the newly created extent. The client caches this information and uses it to send
future writes to the primary server. The stream manager is also
responsible for handling unavailable or faulty extent replicas by
creating new ones and reconfiguring the replication chains they
are part of.

The *partition layer* is where high-level file operations are translated to low-level stream operations. Within this layer, the *partition manager* (yet another control plane) manages a large index of all files
stored in the cluster. Each entry in the index contains metadata
such as account and file name and a pointer to the actual data in
the stream service (list of extent plus offset and length). The partition manager range-partitions the index and maps each partition
to a partition server. The partition manager is also responsible for
load-balancing partitions across servers, splitting partitions when
they become too hot, and merging cold ones.

The partition layer also asynchronously replicates accounts across
clusters in the background. This functionality is used to migrate
accounts from one cluster to another for load-balancing purposes
and disaster recovery.

Finally, the *front-end service* (a reverse proxy) is a stateless service
that authenticates requests and routes them to the appropriate partition server using the mapping managed by the partition manager.

Although we have only coarsely described the architecture of AS,
it’s a great showcase of the scalability patterns applied to a concrete system. As an interesting historical note, AS was built from
the ground up to be strongly consistent, while AWS S3 started offering the same guarantee in 2021.

# Network load balancing
Scaling out (horizontally) a stateless application doesn’t require much effort, *assuming* its dependencies can scale accordingly as well. Scaling out a stateful service,
like a data store, is a lot more challenging since it needs to replicate state and thus requires some form of coordination, which adds
complexity and can also become a bottleneck. As a general rule of
thumb, we should try to keep our applications stateless by pushing state to third-party services designed by teams with years of experience building such services.

Distributing requests across a pool of servers has many benefits.
Because clients are decoupled from servers and don’t need to know
their individual addresses, the number of servers behind the load
balancer can increase or decrease transparently. And since multiple redundant servers can interchangeably be used to handle re-
quests, a load balancer can detect faulty ones and take them out of
the pool, increasing the availability of the overall application.

Distributing requests across a pool of servers has many benefits.
Because clients are decoupled from servers and don’t need to know
their individual addresses, the number of servers behind the load
balancer can increase or decrease transparently. And since multiple redundant servers can interchangeably be used to handle requests, a load balancer can detect faulty ones and take them out of
the pool, increasing the availability of the overall application.

The availability of a system is
the percentage of time it’s capable of servicing requests and doing useful work. Another way of thinking about it is that it’s the
probability that a request will succeed.

The reason why a load balancer increases the theoretical availability is that in order for the application to be considered unavailable,
all the servers need to be down. With N servers, the probability
that they are all unavailable is the product of the servers’ failure
rates. By subtracting this product from 1, we can determine the
theoretical availability of the application.

For example, if we have two servers behind a load balancer and
each has an availability of 99%, then the application has a theoretical availability of 99.99%:
1 − (0.01 ⋅ 0.01) = 0.9999

Intuitively, the nines of independent servers sum up. Thus, in
the previous example, we have two independent servers with two
nines each, for a total of four nines of availability. Of course, this
number is only theoretical because, in practice, the load balancer
doesn’t remove faulty servers from the pool immediately. The formula also naively assumes that the failure rates are independent,
which might not be the case. Case in point: when a faulty server is removed from the load balancer’s pool, the remaining ones might
not be able to sustain the increase in load and degrade.

## Load balancing
The algorithms used for routing requests can vary from *round-robin* to *consistent hashing* to ones that take into account the
servers’ load.

Using cached or otherwise delayed metrics to distribute requests
to servers can result in surprising behaviors. For example, if a
server that just joined the pool reports a load of 0, the load balancer will hammer it until the next time its load is sampled. When
that happens, the server will report that it’s overloaded, and the
load balancer will stop sending more requests to it. This causes the
server to alternate between being very busy and not being busy at
all.
As it turns out, randomly distributing requests to servers without
accounting for their load achieves a better load distribution. Does
that mean that load balancing using delayed load metrics is not
possible? There is a way, but it requires combining load metrics
with the power of randomness. The idea is to randomly pick two
servers from the pool and route the request to the least-loaded one
of the two. This approach works remarkably well in practice.

### Service discovery
Service discovery is the mechanism the load balancer uses to discover the pool of servers it can route requests to. A naive way to
implement it is to use a static configuration file that lists the IP addresses of all the servers, which is painful to manage and keep up
to date.

A more flexible solution is to have a fault-tolerant coordination
service, like, e.g., etcd or Zookeeper, manage the list of servers.
When a new server comes online, it registers itself to the coordination service with a TTL. When the server unregisters itself, or the
TTL expires because it hasn’t renewed its registration, the server
is removed from the pool.

Adding and removing servers dynamically from the load
balancer’s pool is a key functionality cloud providers use to
implement autoscaling, i.e., the ability to spin up and tear down
servers based on load.

### Health checks
A load balancer uses health checks to detect when a server can no
longer serve requests and needs to be temporarily removed from
the pool. There are fundamentally two categories of health checks:
passive and active.

A *passive health check* is performed by the load balancer as it
routes incoming requests to the servers downstream. If a server
isn’t reachable, the request times out, or the server returns a
non-retriable status code (e.g., 503), the load balancer can decide
to take that server out of the pool.

Conversely, an *active health check* requires support from the downstream servers, which need to expose a dedicated health endpoint
that the load balancer can query periodically to infer the server’s
health. The endpoint returns 200 (OK) if the server can serve requests or a 5xx status code if it’s overloaded and doesn’t have more
capacity to serve requests. If a request to the endpoint times out,it also counts as an error.

The endpoint’s handler could be as simple as always returning 200
OK, since most requests will time out when the server is degraded.
Alternatively, the handler can try to infer whether the server is
degraded by comparing local metrics, like CPU usage, available
memory, or the number of concurrent requests being served, with
configurable thresholds.

But here be dragons: if a threshold is misconfigured or the health
check has a bug, all the servers behind the load balancer may fail
the health check. In that case, the load balancer could naively
empty the pool, taking the application down. However, in practice, if the load balancer is “smart enough,” it should detect that a
large fraction of the servers are unhealthy and consider the health
checks to be unreliable. So rather than removing servers from thepool, it should ignore the health checks altogether so that new re-quests can be sent to any server.

Thanks to health checks, the application behind the load balancer
can be updated to a new version without any downtime. During
the update, a rolling number of servers report themselves as unavailable so that the load balancer stops sending requests to them.
This allows in-flight requests to complete (drain) before the servers
are restarted with the new version. More generally, we can use this
mechanism to restart a server without causing harm.

For example, suppose a stateless application has a rare memory
leak that causes a server’s available memory to decrease slowly
over time. When the server has very little physical memory available, it will swap memory pages to disk aggressively. This constant swapping is expensive and degrades the performance of the
server dramatically. Eventually, the leak will affect the majority of
servers and cause the application to degrade.

In this case, we could force a severely degraded server to restart.
That way, we don’t have to develop complex recovery logic when
a server gets into a rare and unexpected degraded mode. Moreover, restarting the server allows the system to self-heal, giving its operators time to identify the root cause.

To implement this behavior, a server could have a separate background thread — a watchdog — that wakes up periodically and
monitors the server’s health. For example, the *watchdog* could
monitor the available physical memory left. When a monitored
metric breaches a specific threshold for some time, the watchdog
considers the server degraded and deliberately crashes or restarts
it.

Of course, the watchdog’s implementation needs to be well-tested
and monitored since a bug could cause servers to restart continuously. That said, I find it uncanny how this simple pattern can
make an application a lot more robust to gray failures.

Load balancing algorithms:
> * Static
>   * Round robin
>   * Weighted Round robin
>   * Sticky Round robin
>   * Hash 
>* Dynamic
>   * Least connection
>   * Least time 

### Static
Distribute requests to servers without taking the account of the servers' real-time conditions and performance metrics. The main advantage is simplicity but the downside is less adaptivity and precision. 
Round robin rotates requests evenly around the servers. It can potentially overload servers if they are not properly monitored. Sticky round robin tries to send subsequent requests from the same user to the same server. The goal is to improve performance by having related data on the same server. But uneven loads can easily occur since newly arriving users are assigned randomly. 

Weighted round robin allows operators to assign different weights or priority to a different server. Servers with higher weights will receive a proportionally higher number of requests. This allows us to account for heterogeneous server capabilities. The downside is that weights must be manually configured, which is less adaptive to real time changes. Hash-based algorithms use a hash function to map incoming requests to the backend servers. The hash function often uses the client's IP address or the requested URL as an input for determining where to route each request. It can evenly distribute requests if the function is chosen wisely. However, selecting an optimal hash function can be challenging. 

### Dynamic
Adapt in real-time by taking active performance metrics and server conditions into account when distributing requests. Least connections algorithms send each new request to the server currently with the least number of active connections or open requests. This requires actively tracking the number of ongoing connections on each backend server. The advantage is new requests are adaptively routed to where there is most remaining capacity. However, it's possible for load to unintentionally concentrate on certain servers if connections pile up unevenly. Least response time algorithms send incoming requests to the server with the lowest current latency or fastest response time. Latency for each server is continuously measured and factored in. This approach is highly adaptive and reactive. However, requires constant monitoring which incurs significant overhead and introduces complexity. It also doesn't consider how many existing requests each server already has. 

## DNS Load balancing
It’s important to have a
basic knowledge of how a load balancer works. Because every request needs to go through it, it contributes to your applications’
performance and availability.

A simple way to implement a load balancer is with DNS. For example, suppose we have a couple of servers that we would like
to load-balance requests over. If these servers have public IP addresses, we can add those to the application’s DNS record and
have the clients pick one when resolving the DNS address.

Although this approach works, it’s not resilient to failures. If one of
the two servers goes down, the DNS server will happily continue
to serve its IP address, unaware that it’s no longer available. Even
if we were to automatically reconfigure the DNS record when a
failure happens and take out the problematic IP, the change needs time to propagate to the clients, since DNS entries are cached.

The one use case where DNS is used in practice to load-balance is
for distributing traffic to different data centers located in different
regions (*global DNS load balancing*). We have already encountered
a use for this when discussing CDNs.

## Transport layer load balancing
A more flexible load-balancing solution can be implemented with
a load balancer that operates at the TCP level of the network stack
(aka L4 load balancer) through which all the traffic between
clients and servers flows.

A network load balancer has one or more physical *network interface cards* mapped to one or more *virtual IP* (VIP) addresses. A VIP, in
turn, is associated with a pool of servers. The load balancer acts
as an intermediary between clients and servers — clients only see
the VIP exposed by the load balancer and have no visibility of the
individual servers associated with it.

When a client creates a new TCP connection with a load balancer’s
VIP, the load balancer picks a server from the pool and henceforth
shuffles the packets back and forth for that connection between the
client and the server. And because all the traffic goes through the
load balancer, it can detect servers that are unavailable (e.g., with a
passive health check) and automatically take them out of the pool,
improving the system’s reliability.

A connection is identified by a tuple (source IP/port, destination
IP/port). Typically, some form of hashing is used to assign a connection tuple to a server that minimizes the disruption caused by
a server being added or removed from the pool, like consistent
hashing.

To forward packets downstream, the load balancer translates each
packet’s source address to the load balancer’s address and its destination address to the server’s address. Similarly, when the load
balancer receives a packet from the server, it translates its source
address to the load balancer’s address and its destination address
to the client’s address.

As the data going out of the servers usually has a greater volume
than the data coming in, there is a way for servers to bypass the
load balancer and respond directly to the clients using a mechanism called direct server return, which can significantly reduce
the load on the load balancer.

A network load balancer can be built using commodity machines and scaled out using a combination of Anycast and ECMP. Load
balancer instances announce themselves to the data center’s edge
routers with the same Anycast VIP and identical BGP weight. Using an Anycast IP is a neat trick that allows multiple machines to
share the same IP address and have routers send traffic to the one
with the lowest BGP weight. If all the instances have the same identical BGP weight, routers use equal-cost multi-path routing (con-
sistent hashing) to ensure that the packets of a specific connection
are generally routed to the same load balancer instance.

Although load balancing connections at the TCP level is very
fast, the drawback is that the load balancer is just shuffling bytes
around without knowing what they actually mean. Therefore,
L4 load balancers generally don’t support features that require
higher-level network protocols, like terminating TLS connections.
A load balancer that operates at a higher level of the network
stack is required to support these advanced use cases.

## Application layer load balancing
An application layer load balancer (aka L7 load balancer) is an
HTTP reverse proxy that distributes requests over a pool of servers.
The load balancer receives an HTTP request from a client, inspects
it, and sends it to a backend server.

There are two different TCP connections at play here, one between
the client and the L7 load balancer and another between the L7
load balancer and the server. Because a L7 load balancer operates
at the HTTP level, it can de-multiplex individual HTTP requests
sharing the same TCP connection. This is even more important
with HTTP 2, where multiple concurrent streams are multiplexed
on the same TCP connection, and some connections can be a lot
more expensive to handle than others.

The load balancer can do smart things with application traffic, like
rate-limit requests based on HTTP headers, terminate TLS connections, or force HTTP requests belonging to the same *logical session*
to be routed to the same backend server. For example, the load balancer could use a cookie to identify which logical session a request
belongs to and map it to a server using consistent hashing. That
allows servers to cache session data in memory and avoid fetching it from the data store for each request. The caveat is that sticky sessions can create hotspots, since some sessions can be much more
expensive to handle than others.

A L7 load balancer can be used as the backend of a L4 load balancer
that load-balances requests received from the internet. Although
L7 load balancers have more capabilities than L4 load balancers,
they also have lower throughput, making L4 load balancers better
suited to protect against certain DDoS attacks, like SYN floods.

A drawback of using a dedicated load balancer is that all the traffic
directed to an application needs to go through it. So if the load
balancer goes down, the application behind it does too. However,
if the clients are internal to the organization, load balancing can be
delegated to them using the *sidecar pattern*. The idea is to proxy all
a client’s network traffic through a process co-located on the same
machine (the sidecar proxy). The sidecar process acts as a L7 load
balancer, load-balancing requests to the right servers. And, since
it’s a reverse proxy, it can also implement various other functions,
such as rate-limiting, authentication, and monitoring.

This approach (aka “service mesh”) has been gaining popularity
with the rise of microservices in organizations with hundreds of
services communicating with each other. As of this writing, popular sidecar proxy load balancers are NGINX, HAProxy, and Envoy. The main advantage of this approach is that it delegates load-
balancing to the clients, removing the need for a dedicated load
balancer that needs to be scaled out and maintained. The drawback is that it makes the system a lot more complex since now we
need a control plane to manage all the sidecars.

### Service mesh
What is it? A configurable infrastructure layer for a microservices application. It provides microservices governance by adding necessary visibility and security controls. The service mesh helps:
- Discover
- Authorize
- Track

### Sidecar pattern
Problem: how to implement related and isolated peripheral features without being tightly integrated into the main application.

Sidecars handle inter service communications with features like:
- Service discovery
- Load balancing
- Encryption
- Authentication and authorization 
- Monitoring and security-related concerns

... anything that can be abstracted away from the individual service. 

Challenges:
- Inter-container communication
- Resource overhead

Use-cases:
- Service mesh
- Security
- Observability