# Introduction
The simplest and quickest way to increase the capacity is to *scale
up* the machine hosting the application.

The alternative to scaling up is to *scale out* by distributing the application across multiple nodes. For example, we
can move the database to a dedicated machine as a first step. By
doing that, we have increased the capacity of both the server and
the database since they no longer have to compete for resources.
This is an example of a more general pattern called *functional decomposition*: breaking down an application into separate components,
each with its own well-defined responsibility.

There are other
two general patterns that we can exploit (and combine) to build
scalable applications: splitting data into partitions and distributing them among nodes (*partitioning*) and replicating functionality
or data across nodes, also known as horizontal scaling (*replication*).

# HTTP caching
A static resource contains data that usually
doesn’t change from one request to another, like a JavaScript or
CSS file. Instead, a dynamic resource is generated by the server
on the fly, like a JSON document containing the user’s profile.

Since a static resource doesn’t change often, it can be cached. The
idea is to have a client (i.e., a browser) cache the resource for some
time so that the next access doesn’t require a network call, reducing
the load on the server and the response time.

HTTP caching is limited to safe request meth-
ods that don’t alter the server’s state, like GET or HEAD. Suppose
a client issues a GET request for a resource it hasn’t accessed before. The local cache intercepts the request, and if it can’t find the
resource locally, it will go and fetch it from the origin server on
behalf of the client.

The server uses specific HTTP response headers to let clients
know that a resource is cachable. So when the server responds with the resource, it adds a *Cache-Control* header that defines for
how long to cache the resource (TTL) and an *ETag* header with a
version identifier. Finally, when the cache receives the response,
it stores the resource locally and returns it to the client.

Now, suppose some time passes and the client tries to access the
resource again. The cache first checks whether the resource hasn’t
expired yet, i.e., whether it’s *fresh*. If so, the cache immediately
returns it. However, even if the resource hasn’t expired from the
client’s point of view, the server may have updated it in the meantime. That means reads are not strongly consistent, but we will
safely assume that this is an acceptable tradeoff for our application.

If the resource has expired, it’s considered stale. In this case, the cache sends a GET request to the server with a conditional header
(like *If-None-Match*) containing the version identifier of the stale
resource to check whether a newer version is available. If there is,
the server returns the updated resource; if not, the server replies
with a 304 Not Modified status code.

Ideally, a static resource should be immutable so that clients can
cache it “forever,” which corresponds to a maximum length of a
year according to the HTTP specification. We can still modify a
static resource if needed by creating a new one with a different
URL so that clients will be forced to fetch it from the server.

Another advantage of treating static resources as immutable is that
we can update multiple related resources atomically. For example,
if we publish a new version of the application’s website, the up-
dated HTML index file is going to reference the new URLs for the
JavaScript and CSS bundles. Thus, a client will see either the old version of the website or the new one depending on which index
file it has read, but never a mix of the two that uses, e.g., the old
JavaScript bundle with the new CSS one.

Another way of thinking about HTTP caching is that we treat
the read path (GET) differently from the write path (POST, PUT,
DELETE) because we expect the number of reads to be several
orders of magnitude higher than the number of writes. This is a
common pattern referred to as the *Command Query Responsibility
Segregation* (CQRS) pattern.

To summarize, allowing clients to cache static resources has reduced the load on our server, and all we had to do was to play
with some HTTP headers! We can take caching one step further
by introducing a server-side HTTP cache with a reverse proxy.

## Reverse proxies
A reverse proxy is a server-side proxy that intercepts all communications with clients. Since the proxy is indistinguishable from
the actual server, clients are unaware that they are communicating
through an intermediary.

A common use case for a reverse proxy is to cache static resources
returned by the server. Since the cache is shared among the clients,
it will decrease the load of the server a lot more than any client-side
cache ever could.

Because a reverse proxy is a middleman, it can be used for much
more than just caching. For example, it can:
* authenticate requests on behalf of the server;
* compress a response before returning it to the client to speed
up the transmission;
* rate-limit requests coming from specific IPs or users to protect the server from being overwhelmed;
* load-balance requests across multiple servers to handle more
load.

NGINX and HAProxy are widely-used reverse proxies that we could
leverage to build a server-side cache. However, many use cases for
reverse proxies have been commoditized by managed services. So,
rather than building out a server-side cache, we could just leverage
a Content Delivery Network (CDN).

| Aspect               | Proxy                                    | Reverse Proxy                                     |
|----------------------|------------------------------------------|---------------------------------------------------|
| **Definition**       | Forwards client requests to the internet, masking client identity. | Sits between clients and servers, forwarding requests from clients to backend servers. |
| **Primary Use**      | Used by clients to hide their IP addresses, access restricted content, or add anonymity. | Used by servers to load balance, improve security, and handle client requests efficiently. |
| **Location**         | Positioned between the client and external network (e.g., the internet). | Positioned between the internet (or clients) and internal servers. |
| **Example Use Cases**| Web browsing with anonymity, accessing geo-restricted content. | Load balancing, caching, SSL termination, hiding internal server structure. |
| **Request Flow**     | Client -> Proxy -> Server (Internet) -> Proxy -> Client | Client -> Reverse Proxy -> Server -> Reverse Proxy -> Client |
| **IP Address Seen by Server** | Proxy’s IP address (masks client's IP) | Reverse proxy’s IP address (client’s IP is hidden) |
| **Common Technologies** | Squid, Privoxy | Nginx, HAProxy, Apache HTTP Server |
| **Security Role**    | Primarily focused on client privacy | Adds a security layer by controlling and filtering incoming requests to servers |

The difference between a forward and reverse proxy is subtle but important. A simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.

# Content delivery networks
A CDN is an overlay network of geographically distributed
caching servers (reverse proxies) architected around the design
limitations of the network protocols that run the internet.

When using a CDN, clients hit URLs that resolve to caching servers
that belong to the CDN. When a CDN server receives a request,
it checks whether the requested resource is cached locally. If not,
the CDN server transparently fetches it from the origin server (i.e.,
our application server) using the original URL, caches the response
locally, and returns it to the client. AWS CloudFront and Akamai
are examples of well-known CDN services.

## Overlay network
You would think that the main benefit of a CDN is caching, but it’s
actually the underlying network substrate. The public internet is
composed of thousands of networks, and its core routing protocol,
BGP, was not designed with performance in mind. It primarily uses the number of hops to cost how expensive a path is with respect to another, without considering their latencies or congestion.

As the name implies, a CDN is a network. More specifically, an
overlay network built on top of the internet that exploits a variety
of techniques to reduce the response time of network requests and
increase the bandwidth of data transfers.

No matter how fast the server is, if the client is located on the other
side of the world from it, the response time is going to be over 100
ms just because of the network latency, which is physically limited
by the speed of light. Not to mention the increased error rate when
sending data across the public internet over long distances.

This is why CDN clusters are placed in multiple geographical loca-
tions to be closer to clients. But how do clients know which cluster
is closest to them? One way is via *global DNS load balancing*: an
extension to DNS that considers the location of the client inferred
from its IP, and returns a list of the geographically closest clusters
taking into account also the network congestion and the clusters’
health.

CDN servers are also placed at *internet exchange points*, where ISPs
connect to each other. That way, virtually the entire communication from the origin server to the clients flows over network links
that are part of the CDN, and the brief hops on either end have low
latencies due to their short distance.

The routing algorithms of the overlay network are optimized
to select paths with reduced latencies and congestion, based
on continuously updated data about the health of the network.
Additionally, TCP optimizations are exploited where possible,
such as using pools of persistent connections between servers to avoid the overhead of setting up new connections and using
optimal TCP window sizes to maximize the effective bandwidth.

The overlay network can also be used to speed up the delivery
of dynamic resources that cannot be cached. In this capacity, the
CDN becomes the frontend for the application, shielding it against
distributed denial-of-service (DDoS) attacks.

## Caching
A CDN can have multiple content caching layers. The top layer is
made of edge clusters deployed at different geographical locations,
as mentioned earlier. But infrequently accessed content might not
be available at the edge, in which case the edge servers must fetch it
from the origin server. Thanks to the overlay network, the content
can be fetched more efficiently and reliably than what the public
internet would allow.

There is a tradeoff between the number of edge clusters and the
cache hit ratio, i.e., the likelihood of finding an object in the cache. The higher the number of edge clusters, the more geographically
dispersed clients they can serve, but the lower the cache hit ratio
will be, and consequently, the higher the load on the origin server.
To alleviate this issue, the CDN can have one or more intermedi-
ary caching clusters deployed in a smaller number of geographical
locations, which cache a larger fraction of the original content.

Within a CDN cluster, the content is partitioned among multiple
servers so that each one serves only a specific subset of it; this is
necessary as no single server would be able to hold all the data.

# Partitioning
When an application’s data keeps growing in size, its volume will
eventually become large enough that it can’t fit on a single machine. To work around that, it needs to be split into partitions, or
*shards*, small enough to fit into individual nodes. As an additional
benefit, the system’s capacity for handling requests increases as
well, since the load of accessing the data is spread over more nodes.

When a client sends a request to a partitioned system, the request
needs to be routed to the node(s) responsible for it. A gateway service (i.e., a reverse proxy) is usually in charge of this, knowing how
the data is mapped to partitions and nodes. This
mapping is generally maintained by a fault-tolerant coordination
service, like etcd or Zookeeper.

Partitioning is not a free lunch since it introduces a fair amount of
complexity:
* A gateway service is required to route requests to the right
nodes.
* To roll up data across partitions, it needs to be pulled from
multiple partitions and aggregated (e.g., think of the complexity of implementing a “group by” operation across partitions).
* Transactions are required to atomically update data that
spans multiple partitions, limiting scalability.
* If a partition is accessed much more frequently than others,
the system’s ability to scale is limited.
* Adding or removing partitions at run time becomes challenging, since it requires moving data across nodes.

It’s no coincidence we are talking about partitioning right after discussing CDNs. A cache lends itself well to partitioning as it avoids
most of this complexity. For example, it generally doesn’t need to
atomically update data across partitions or perform aggregations
spanning multiple partitions.

Now that we have an idea of what partitioning is and why it’s
useful, let’s discuss how data, in the form of key-value pairs, can be
mapped to partitions. At a high level, there are two main ways of
doing that, referred to as *range partitioning* and *hash partitioning*. An
important prerequisite for both is that the number of possible keys
is very large; for example, a boolean key with only two possible
values is not suited for partitioning since it allows for at most two partitions.

## Range partitioning
Range partitioning splits the data by key range into lexicographically sorted partitions. To make range
scans fast, each partition is generally stored in sorted order on disk.

The first challenge with range partitioning is picking the partition
boundaries. For example, splitting the key range evenly makes
sense if the distribution of keys is more or less uniform. If not, like
in the English dictionary, the partitions will be unbalanced and
some will have a lot more entries than others.

Another issue is that some access patterns can lead to *hotspots*,
which affect performance. For example, if the data is range
partitioned by date, all requests for the current day will be served
by a single node. There are ways around that, like adding a
random prefix to the partition keys, but there is a price to pay in
terms of increased complexity.

When the size of the data or the number of requests becomes too
large, the number of nodes needs to be increased to sustain the increase in load. Similarly, if the data shrinks and/or the number
of requests drops, the number of nodes should be reduced to decrease costs. The process of adding and removing nodes to balance the system’s load is called *rebalancing*. Rebalancing has to be implemented in a way that minimizes disruption to the system, which
needs to continue to serve requests. Hence, the amount of data
transferred when rebalancing partitions should be minimized.

One solution is to create a lot more partitions than necessary when
the system is first initialized and assign multiple partitions to each
node. This approach is also called *static partitioning* since the number of partitions doesn’t change over time. When a new node joins,
some partitions move from the existing nodes to the new one so
that the store is always in a balanced state. The drawback of this
approach is that the number of partitions is fixed and can’t be easily changed. Getting the number of partitions right is hard — too
many partitions add overhead and decrease performance, while
too few partitions limit scalability. Also, some partitions might
end up being accessed much more than others, creating hotspots.

The alternative is to create partitions on demand, also referred to
as *dynamic partitioning*. The system starts with a single partition,
and when it grows above a certain size or becomes too hot, the
partition is split into two sub-partitions, each containing approximately half of the data, and one of the sub-partitions is transferred
to a new node. Similarly, when two adjacent partitions become
small or “cold” enough, they can be merged into a single one.

## Hash partitioning
Let’s take a look at an alternative way of mapping data to partitions. The idea is to use a hash function that deterministically
maps a key (string) to a seemingly random number (a hash) within
a certain range (e.g., 0 and 2^64 − 1). This guarantees that the keys’
hashes are distributed uniformly across the range.

Next, we assign a subset of the hashes to each partition. For example, one way of doing that is by taking the
modulo of a hash, i.e., hash(key) mod N, where N is the number of
partitions.

Although this approach ensures that the partitions contain more or less the same number of entries, it doesn’t eliminate hotspots
if the *access pattern* is not uniform. For example, if a single key is
accessed significantly more often than others, the node hosting the
partition it belongs to could become overloaded. In this case, the
partition needs to be split further by increasing the total number
of partitions. Alternatively, the key needs to be split into sub-keys
by, e.g., prepending a random prefix.

Assigning hashes to partitions via the modulo operator can be-
come problematic when a new partition is added, as most keys
have to be moved (or shuffled) to a different partition because their
assignment changed. Shuffling data is extremely expensive as it
consumes network bandwidth and other resources. Ideally, if a
partition is added, only 𝐾/𝑁 keys should be shuffled around, where
𝐾 is the number of keys and 𝑁 is the number of partitions. One
widely used hashing strategy with that property is consistent hashing.

With *consistent hashing*, a hash function randomly maps both the
partition identifiers and keys onto a circle, and each key is assigned
to the closest partition that appears on the circle in clockwise order.

Now, when a new partition is added, only the keys that now map to it on the circle need to be reassigned.

The main drawback of hash partitioning compared to range partitioning is that the sort order over the partitions is lost, which is
required to efficiently scan all the data in order. However, the data
within an individual partition can still be sorted based on a secondary key.

